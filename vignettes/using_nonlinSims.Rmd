---
title: "Using nonlinSims"
author: "Sebastian Sciarra"
date: "September 15, 2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using nonlinSims}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(easypackages)
packages <- c('tidyverse', 'data.table', "OpenMx", 'devtools')
libraries(packages)

#packages to load: tidyverse, data.table, 
```


# Experiment 1 (Does equal spacing perform best across all patterns of s-shaped change?)

To test whether equal spacing should be used when one does not know the pattern of s-shape change, Experiment 1 will manipulate the 
pattern of change, measurement spacing, and the number of measurements. Measurement spacing will be manipulated such that measurements
are either separated by equal-length intervals, intervals that increase in length over time (*time-increasing spacing*), intervals that
decrease in length over time (*time-decreasing spacing*), or intervals that separate measurements at the beginning, middle, and end of
the measurement window (*middle-and-extreme spacing*). Number of measurements will be manipulated to either be 5, 7, 9, or 11 
measurements. The patterns of s-shape change will be manipulated by modulating the midpoint parameters, thus shifting the inflection 
point. Change will be assumed to occur over a 360-day period. Sample size will be fixed at 225. 

```{r logistic-change-patterns, echo=F}

theta <- 3.5
alpha <- 3.98
diff <- alpha - theta
beta <- 60
gamma <- 30

days <- seq(from = 0, to = 364, length.out = 365)
midway_point <- beta-1
satiation_point <- beta+gamma-1 

s_shape_nonlin <- data.frame('curve_score' = theta + ((alpha - theta)/(1 + exp((beta - days)/gamma))), 
                      'day' = days)

s_shape_nonlin_3 <- data.frame('curve_score' = diff/(1 + exp((beta - days)/gamma)) + theta, 
                      'day' = days)

midway_value <- s_shape_nonlin$curve_score[which(s_shape_nonlin$day == midway_point)]
satiation_value <- s_shape_nonlin$curve_score[which(s_shape_nonlin$day == satiation_point)]
starting_value <- s_shape_nonlin$curve_score[s_shape_nonlin$day == 0]
end_value <- s_shape_nonlin$curve_score[s_shape_nonlin$day == nrow(s_shape_nonlin)-1]

base_nonlin_plot <- ggplot(s_shape_nonlin_3, aes(x = day, y = curve_score)) + 
  geom_line(size = 1) + 
  theme_classic(base_family = 'Helvetica') +
  scale_y_continuous(limits = c(3.5, 4.0), breaks = seq(from = 3.5, to = 4.0, by = 0.1)) +
  scale_x_continuous(breaks = seq(0, 365, by = 45), limits = c(0, 365), minor_breaks = seq(0, 365, by = 45))+
  labs(x = 'Day', y = 'Population value', size = 16) +
  annotate(geom = 'text', x = 54, y = 3.9, label = 'y == theta + frac(alpha - theta, 1 + e^(frac(beta-time, gamma)))', parse = T, size = 5) + 
  theme(axis.text = element_text(size = 11)) 

base_nonlin_plot
```

```{r generate-thee-param-data, echo=F}
#fixed effects
sd_scale <- 1.5
common_effect_size <- 0.32
diff_fixed <- sd_scale * common_effect_size

num_measurements <- 7
time_period <- 360
scaling_factor <- time_period/num_measurements

gamma_fixed <- 20

#random effects 
sd_diff <- 0.15
sd_beta <- 10
sd_gamma <- 5
sd_error <- 0.05

#correlations
cor_diff_beta <- 0
cor_diff_gamma <- 0
cor_beta_gamma <- 0
scaling_constant <- 3
                              

#INDEPENDENT VARIABLE SETUP (note that sample size is fixed at 225)
##Set beta values at 80, 180, and 280 
beta_fixed <- seq(from = 80, to = 280, by = 100)[1]

#Set number of measurements to either 5, 7, 9, or 11
num_measurements <- seq(from = 5, to = 11, by = 2)[1]

##Iterate through all four spacing conditions: equal, time_inc, time_dec, and middle_ext
schedule <- compute_measurement_schedule(time_period = 360, num_measurements = num_measurements, base_time_length = 30, measurement_spacing = 'time_inc')

#Set up of population-level parameters
pop_params <- generate_three_param_pop_curve(diff_fixed = diff_fixed, beta_fixed = beta_fixed, gamma_fixed = gamma_fixed, 
                               sd_diff = sd_diff, sd_beta = sd_beta, sd_gamma = sd_gamma, sd_error = sd_error, 
                               cor_diff_beta = cor_diff_beta, cor_diff_gamma = cor_diff_gamma, cor_beta_gamma = cor_beta_gamma,
                               scaling_constant = scaling_constant)

cov_matrix <- generate_three_param_cov_matrix(num_time_points = num_measurements, pop_param_list = pop_params)

#Check convergence success on all conditions and set number of iterations to 100 (instead of 1000 as in experiment). THis will run 3 x 4 x 4 = 1200 iterations 

factor_list <- list(spacing = c("equal", "time_inc", "time_dec", "mid_ext"),
                      num_measurements = seq(from = 5, to = 11, by = 2), 
                      midpoint =  seq(from = 80, to = 280, by = 100))

test_convergence(factor_list = factor_list, num_iterations = 1, pop_params = pop_params, response_group_size = 225)

```


```{r echo=F}
schedule <- compute_measurement_schedule(time_period = 360, num_measurements = 11, base_time_length = 30, measurement_spacing = 'time_inc')
cov_matrix <- generate_three_param_cov_matrix(num_time_points = 11, pop_param_list = pop_params)

param_table <- generate_ind_param_values(pop_param_list = pop_params, 
                                           response_group_size = 100, 
                                           num_time_points = 11, 
                                           cov_matrix = cov_matrix)
  
data <- generate_group_scores(num_measurements = 11, 
                                param_table = param_table,measurement_days = schedule$measurement_days, 
                                time_period = 360, scaling_constant = 3)
  
data_wide <- pivot_wider(data = data[ ,1:3], names_from = 'measurement_day', values_from = 'obs_score', 
                           names_prefix = sprintf('t%d_', 1:uniqueN(data$measurement_day)))


create_logistic_growth_model(data_wide = data_wide, model_name = 'nl')
names(data_wide)[-1] <- generate_manifest_var_names(data_wide)
load_all()

```


```{r errors-to-fix}
#1. Even number of measurements for compute measurement schedule. 
#2. 

```


# Code tests 

```{r starting-value-procedure, echo=F}
#Fixed_effect starting values  
#In MPlus framework, w represents starting values (pg. 520 of manual; defaults for growth models)
##fixed-effect values for  are obtained using self-starting function and not fitting any nonlinear regression model to the data (fitting a 
#model at this stage can yield impossible values). Also note that starting value for diff_fixed is calculated by subtracting mean ending value from mean
#beginning value
##In Mplus, automatic starting values for the growth factor means and variances are generated based on individual regressions of the outcome variable on time. 
##Unfortunately, following this approach in this specific situation with nonlinear models can hijacks the starting value procedure (see above comment). 
fixed_effect_starts <- logistic_self_start(data = data)

#Random-effect starting values 
##diff_rand is obtained by obtaining variance of change scores from each person (i.e., var((score_final_time_point - score_final_time_point))
diff_rand_start <- compute_diff_rand_start(data)

rand_starts <- compute_rand_effect_starts(data = data,time_rand_effects = c('beta_rand', 'gamma_rand'))
fixed_starts <- compute_fixed_effect_starts(data = data, rand_effect_starts = rand_starts)


#w  + srb
##r = unif(-0,5, 0.5) 
##s = 5 (determines strength of random perturbation)
##b = base scale variable = 2max{sqrt(var), 1}, where var = starting value for variance of parameter
###b for diff_fixed = 2 (almost all of the time)
###b for beta_fixed = sqrt(var) almost all of the time 
###b for gamma_fixed = sqrt(var) almost all of the time
###b for random effects and residual variance = 0 (i.e., no perturbation in generation process of random variables or residual variance)

#Residual variance starting value (Hipp & Bauer, 2006)
##20-80% of observed variance (averaged across each time point)
res_var <- compute_residual_start_value(data_wide = data_wide)

```

```{r echo=F}
num_flips <- 10
prob_success <- 0.2
num_successes <- 2

(factorial(num_flips)/(factorial(num_successes)*factorial(num_flips-num_successes)))*prob_success^num_successes*(1-prob_success)^(num_flips-num_successes)


factorial(num_flips)/(factorial(num_successes)*factorial(num_flips-num_successes)) #represents total number of ways two sucesses can be obtained 
prob_success^num_successes #represents probability of having given probability of success on all successful trials 
prob_success^num_successes*(1-prob_success) #represents probability of having given probability of failure on all failed trials 

#solving for the probability of success
num_flips <- 10
num_successes <- 7
prob_success <- 0.9

(factorial(num_flips)/(factorial(num_successes)*factorial(num_flips-num_successes)))*prob_success^num_successes*(1-prob_success)^(num_flips-num_successes)


```


# Appendix 
## Apppendix A: Computation of spacing for time-increasing and time-decreasing conditions 

Several attempts were made to compute interval lengths such that they were of integer lengths. Unfortunately, it was not possible to find a set of intervals that followed the equation
$i_{length} = kc + b$ (where $c$ represents the constant length and $b$ represents the base length) such that all interval lengths were of integer values. The code below shows that, 
for all base lengths of 1--30 days (assuming a time period of 360 days), no set of intervals is only integes. 

```{r time_inc-interval-calculations, echo=T}

compute_time_increasing_intervals <- function(time_period, num_measurements, base_time_length) {

  #compute length of constant by first calculating how many days remain after subtracting base_time_length for each interval.
  ##num_measurements-1 = number of intervals
  remaining_num_days <- time_period - (num_measurements-1)*base_time_length
  ##The number of constants = num_measurements - 1
  constant_length <- remaining_num_days/sum(seq(0,(num_measurements-2)))

  interval_lengths <- seq(0,(num_measurements-2))*constant_length + base_time_length
  return(interval_lengths)
}

testx <- function() {
  for (base_time_length in seq(1:30)) {
    print(base_time_length)
    print("=================")
    for (num_measurements in c(5, 7, 9, 11)) {
      xo <- get_stuff(n = n, x = x)
      xo <- c(n, xo, sum(xo))
      print(sprintf("%1.3f",xo))
    }
    print("=================")
  }
}




```

## Appendix B: Other functions

```{r other-functions, echo=F}

```

## Appendix C: 
