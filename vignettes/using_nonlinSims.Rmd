---
title: "Using nonlinSims"
author: "Sebastian Sciarra"
date: "September 15, 2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using nonlinSims}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(devtools)
#install_github("sciarraseb/nonlinSims", force=T)

#library(devtools)
library(easypackages)
packages <- c('devtools', 'nonlinSims', 'parallel', 'tidyverse', "OpenMx", "data.table", 'nlme')
libraries(packages)

#packages to load: tidyverse, data.table, 
```


# Experiment 1 (Does equal spacing perform best across all patterns of s-shaped change?)

To test whether equal spacing should be used when one does not know the pattern of s-shape change, Experiment 1 will manipulate the 
pattern of change, measurement spacing, and the number of measurements. Measurement spacing will be manipulated such that measurements
are either separated by equal-length intervals, intervals that increase in length over time (*time-increasing spacing*), intervals that
decrease in length over time (*time-decreasing spacing*), or intervals that separate measurements at the beginning, middle, and end of
the measurement window (*middle-and-extreme spacing*). Number of measurements will be manipulated to either be 5, 7, 9, or 11 
measurements. The patterns of s-shape change will be manipulated by modulating the midpoint parameters, thus shifting the inflection 
point. Change will be assumed to occur over a 360-day period. Sample size will be fixed at 225. 

```{r generate-four-param-data, echo=F}
time_period <- 360

#fixed effects
sd_scale <- 1
common_effect_size <- 0.32
theta_fixed <- 3
alpha_fixed <-theta_fixed + common_effect_size
gamma_fixed <- 20

#random effects 
sd_theta <- 0.05
sd_alpha <- 0.05
sd_beta <- 10
sd_gamma <- 4
sd_error <- 0.03

num_measurements <- 5
response_group_size <- 225
beta_fixed <- 80

#Set up of population-level parameters
pop_params_4l <- generate_four_param_pop_curve(theta_fixed =  theta_fixed, alpha_fixed = alpha_fixed, 
                                               beta_fixed = beta_fixed, gamma_fixed = gamma_fixed, 
                                               sd_theta = sd_theta, sd_alpha = sd_alpha, 
                                               sd_beta = sd_beta, sd_gamma = sd_gamma, sd_error = sd_error) 

factor_list <- list('num_measurements' = seq(from = 5, to = 5, by = 2), 
                    'spacing' = c('equal', 'time_inc', 'time_dec', 'mid_ext'), 
                    'midpoint' = seq(from = 80, to = 80, by = 100))

sample_output <- run_exp_simulation_4l(factor_list = factor_list, num_iterations = 10, pop_params = pop_params_4l, response_group_size = 50, num_cores = 3, seed = 27)


```


```{r presentation_figures, echo=F}
num_measurements <- 360

beta_fixed <- 280
measurement_days <- seq(from = 1, to = 36, by = 0.5)
change_pattern <- 100/(1 + exp((8 - measurement_days)/2.8))

generate_exponential_response_curve <- function(a, b, c, time_values) {
  curve_values <- a + b*exp(c*time_values)
  
  response_pattern_df <- data.frame('time' = time_values, 'curve_value' = curve_values)
  return(response_pattern_df)
}

response_pattern_df <- generate_exponential_response_curve(a = 100, b = -100, c = -0.272, time_values = 0:89)

#gradient_block <- readPNG(source = 'Figures/manually_created_figures/response_gradient.png')

response_curve <- ggplot(response_pattern_df, aes(x = time, y = curve_value)) + 
  geom_line() + 
  theme_classic(base_family = 'Helvetica') + 
  scale_y_continuous(name = 'Cumulative response percentage') + 
  scale_x_continuous(name = '', breaks = NULL) + 
  #annotate(geom = 'text', label = "y == italic(a + be^ct)", x = 50, y =75, size = 5, parse=T) + 
  theme(axis.text = element_text(size = 10, color = 'black'), axis.title = element_text(size = 15))


ggplot(nonlin_data, aes(x = Day, y = score)) + 
  geom_line(size = 1) +
  scale_x_continuous() +
  scale_y_continuous(name = 'Cumulative response percentage', breaks = seq(0, 1, 0.1)) 
  theme_classic() 
  labs(x = '', y = 'Cumulative resposne percentage') 
```
cov_matrix <- generate_four_param_cov_matrix(num_time_points = num_measurements, pop_param_list = pop_params_4l)


ind_param_values <- generate_4l_ind_param_values(pop_param_list = pop_params_4l, cov_matrix = cov_matrix, response_group_size = 225, num_time_points = num_measurements)
schedule <- compute_measurement_schedule(time_period = 360, num_measurements = num_measurements, smallest_int_length = 30, measurement_spacing = 'equal')

ind_scores <- generate_group_scores_4l(param_table = ind_param_values, num_measurements = num_measurements, measurement_days = schedule$measurement_days, time_period = 360)



ggplot(nonlin_data, aes(x = measurement_days, y = change_pattern)) + 
  geom_line(size = 1) + 
  geom_line(data = ind_scores, size = 0.3, alpha = 0.1, aes(x = measurement_day, y = true_score, group = ID))+ 
  theme_classic() + 
  labs(x = '', y = '') + 
  theme(axis.ticks = element_blank(), 
        axis.text = element_blank())


```



```{r generate-thee-param-data, echo=F}

#fixed effects
sd_scale <- 1
common_effect_size <- 0.32
diff_fixed <- sd_scale * common_effect_size

num_measurements <- 9
time_period <- 360

gamma_fixed <- 20

#random effects 
sd_diff <- 0.15
sd_beta <- 10
sd_gamma <- 3
sd_error <- 0.05

num_time_points <- 5
response_group_size <- 225
beta_fixed <- 80
                        start_values <- list('theta_fixed' = 3, 
                     'alpha_fixed' = 3.5, 
                     'beta_fixed' = 75, 
                     'gamma_fixed' = 23,
                       
                     'theta_rand' = 0.1, #var(middle_95_diff), 
                     'alpha_rand' = 0.1, #var(middle_95_diff), 
                     'beta_rand' = 10, 
                     'gamma_rand' = 3, 
                     'epsilon' = runif(n = 1, min = 1, max = 5))

logistic_model_4l <- create_logistic_growth_model_4l(data_wide = data_wide, model_name = 'test_model', starting_values = start_values)
output <- mxTryHard(model = logistic_model_4l)
logistic_model_4l <- mxAutoStart(logistic_model_4l)
output <- mxTryHard(model = logistic_model_4l)
output$output$estimate



 


```





# Code tests 

```{r starting-value-procedure, echo=F}
#Fixed_effect starting values  
#In MPlus framework, w represents starting values (pg. 520 of manual; defaults for growth models)
##fixed-effect values for  are obtained using self-starting function and not fitting any nonlinear regression model to the data (fitting a 
#model at this stage can yield impossible values). Also note that starting value for diff_fixed is calculated by subtracting mean ending value from mean
#beginning value
##In Mplus, automatic starting values for the growth factor means and variances are generated based on individual regressions of the outcome variable on time. 
##Unfortunately, following this approach in this specific situation with nonlinear models can hijacks the starting value procedure (see above comment). 
fixed_effect_starts <- logistic_self_start(data = data)

#Random-effect starting values 
##diff_rand is obtained by obtaining variance of change scores from each person (i.e., var((score_final_time_point - score_final_time_point))
diff_rand_start <- compute_diff_rand_start(data)

rand_starts <- compute_rand_effect_starts(data = data,time_rand_effects = c('beta_rand', 'gamma_rand'))
fixed_starts <- compute_fixed_effect_starts(data = data, rand_effect_starts = rand_starts)


#w  + srb
##r = unif(-0,5, 0.5) 
##s = 5 (determines strength of random perturbation)
##b = base scale variable = 2max{sqrt(var), 1}, where var = starting value for variance of parameter
###b for diff_fixed = 2 (almost all of the time)
###b for beta_fixed = sqrt(var) almost all of the time 
###b for gamma_fixed = sqrt(var) almost all of the time
###b for random effects and residual variance = 0 (i.e., no perturbation in generation process of random variables or residual variance)

#Residual variance starting value (Hipp & Bauer, 2006)
##20-80% of observed variance (averaged across each time point)
res_var <- compute_residual_start_value(data_wide = data_wide)

```

```{r echo=F}
num_flips <- 10
prob_success <- 0.2
num_successes <- 2

(factorial(num_flips)/(factorial(num_successes)*factorial(num_flips-num_successes)))*prob_success^num_successes*(1-prob_success)^(num_flips-num_successes)


factorial(num_flips)/(factorial(num_successes)*factorial(num_flips-num_successes)) #represents total number of ways two sucesses can be obtained 
prob_success^num_successes #represents probability of having given probability of success on all successful trials 
prob_success^num_successes*(1-prob_success) #represents probability of having given probability of failure on all failed trials 

#solving for the probability of success
num_flips <- 10
num_successes <- 7
prob_success <- 0.9

(factorial(num_flips)/(factorial(num_successes)*factorial(num_flips-num_successes)))*prob_success^num_successes*(1-prob_success)^(num_flips-num_successes)


```


# Appendix 
## Apppendix A: Computation of spacing for time-increasing and time-decreasing conditions 

Several attempts were made to compute interval lengths such that they were of integer lengths. Unfortunately, it was not possible to find a set of intervals that followed the equation
$i_{length} = kc + b$ (where $c$ represents the constant length and $b$ represents the base length) such that all interval lengths were of integer values. The code below shows that, 
for all base lengths of 1--30 days (assuming a time period of 360 days), no set of intervals is only integes. 

```{r time_inc-interval-calculations, echo=T}

compute_time_increasing_intervals <- function(time_period, num_measurements, base_time_length) {

  #compute length of constant by first calculating how many days remain after subtracting base_time_length for each interval.
  ##num_measurements-1 = number of intervals
  remaining_num_days <- time_period - (num_measurements-1)*base_time_length
  ##The number of constants = num_measurements - 1
  constant_length <- remaining_num_days/sum(seq(0,(num_measurements-2)))

  interval_lengths <- seq(0,(num_measurements-2))*constant_length + base_time_length
  return(interval_lengths)
}

testx <- function() {
  for (base_time_length in seq(1:30)) {
    print(base_time_length)
    print("=================")
    for (num_measurements in c(5, 7, 9, 11)) {
      xo <- get_stuff(n = n, x = x)
      xo <- c(n, xo, sum(xo))
      print(sprintf("%1.3f",xo))
    }
    print("=================")
  }
}




```

## Appendix B: Other functions

```{r other-functions, echo=F}

```

## Appendix C: 
